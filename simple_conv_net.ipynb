{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some helper functions.....\n",
    "def im2col_sliding(image, filter_height=3, filter_width=3, \n",
    "                   padding=0, stride=1):\n",
    "    \n",
    "    M, C, h, w, = image.shape\n",
    "    x_padded = np.pad(image, ((0, 0), (0, 0), (padding, padding), (padding, padding)), \n",
    "                      mode='constant')    \n",
    "    h_new = int((h - filter_height + 2*padding) / stride + 1)\n",
    "    w_new = int((w - filter_width + 2*padding) / stride + 1)\n",
    "    \n",
    "    output_vectors = np.zeros((filter_width*filter_height*C, M*h_new*w_new), dtype=image.dtype)\n",
    "    \n",
    "    itr = 0\n",
    "    for i in range(h_new):\n",
    "        for j in range(w_new):\n",
    "             for m in range(M):\n",
    "                    start_i = stride * i\n",
    "                    end_i = stride * i + filter_width\n",
    "                    start_j = stride * j\n",
    "                    end_j = stride * j + filter_height\n",
    "                    output_vectors[:, itr] = x_padded[m, :, start_i:end_i, start_j:end_j].ravel()\n",
    "                    itr += 1                    \n",
    "    return output_vectors\n",
    "\n",
    "def col2img_sliding(cols,  x_shape, filter_height=3, filter_width=3, \n",
    "                    padding=0, stride=1):\n",
    "    N, C, H, W = x_shape\n",
    "    H_padded, W_padded = H + 2 * padding, W + 2 * padding\n",
    "    x_padded = np.zeros((N, C, H_padded, W_padded), dtype=cols.dtype)\n",
    "    \n",
    "    idx = 0\n",
    "    for i in range(0, H_padded - filter_height + 1, stride):\n",
    "        for j in range(0, W_padded - filter_width + 1, stride):\n",
    "            for m in range(N):\n",
    "                col = cols[:, idx]\n",
    "                col = col.reshape((C, filter_height, filter_width))            \n",
    "                x_padded[m, :, i:i+filter_height, j:j+filter_width] += col  \n",
    "                idx += 1\n",
    "    if padding > 0:\n",
    "        return x_padded[:, :, padding:-padding, padding:-padding]\n",
    "    else:\n",
    "        return x_padded\n",
    "\n",
    "# gradient checking utilities....\n",
    "def eval_numerical_gradient_array(f, x, df, h=1e-5):\n",
    "    \"\"\"\n",
    "    Evaluate a numeric gradient for a function that accepts a numpy\n",
    "    array and returns a numpy array.\n",
    "    \"\"\"\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h\n",
    "        pos = f(x).copy()\n",
    "        x[ix] = oldval - h\n",
    "        neg = f(x).copy()\n",
    "        x[ix] = oldval\n",
    "        grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n",
    "        it.iternext()\n",
    "    return grad\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "\n",
    "def conv_forward(X, W, b, stride=1, padding=1):\n",
    "    cache = W, b, stride, padding\n",
    "    n_filters, d_filter, h_filter, w_filter = W.shape\n",
    "    n_x, d_x, h_x, w_x = X.shape\n",
    "    h_out = (h_x - h_filter + 2 * padding) / stride + 1\n",
    "    w_out = (w_x - w_filter + 2 * padding) / stride + 1\n",
    "\n",
    "    if not h_out.is_integer() or not w_out.is_integer():\n",
    "        raise Exception('Invalid output dimension!')\n",
    "\n",
    "    h_out, w_out = int(h_out), int(w_out)\n",
    "\n",
    "    X_col = im2col_sliding(X, h_filter, w_filter, padding=padding, stride=stride)\n",
    "    W_col = W.reshape(n_filters, -1)\n",
    "\n",
    "    out = W_col @ X_col + b\n",
    "    out = out.reshape(n_filters, h_out, w_out, n_x)\n",
    "    out = out.transpose(3, 0, 1, 2)\n",
    "\n",
    "    cache = (X, W, b, stride, padding, X_col)\n",
    "\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def conv_backward(dout, cache):\n",
    "    X, W, b, stride, padding, X_col = cache\n",
    "    n_filter, d_filter, h_filter, w_filter = W.shape\n",
    "\n",
    "    db = np.sum(dout, axis=(0, 2, 3))\n",
    "    db = db.reshape(n_filter, -1)\n",
    "\n",
    "    dout_reshaped = dout.transpose(1, 2, 3, 0).reshape(n_filter, -1)\n",
    "    dW = dout_reshaped @ X_col.T\n",
    "    dW = dW.reshape(W.shape)\n",
    "\n",
    "    W_reshape = W.reshape(n_filter, -1)\n",
    "    dX_col = W_reshape.T @ dout_reshaped\n",
    "    dX = col2im_indices(dX_col, X.shape, h_filter, w_filter, padding=padding, stride=stride)\n",
    "\n",
    "    return dX, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# affine layer...\n",
    "\n",
    "def affine_forward(x, w, b):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for an affine (fully-connected) layer.\n",
    "\n",
    "    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n",
    "    examples, where each example x[i] has shape (d_1, ..., d_k). We will\n",
    "    reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n",
    "    then transform it to an output vector of dimension M.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
    "    - w: A numpy array of weights, of shape (D, M)\n",
    "    - b: A numpy array of biases, of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: output, of shape (N, M)\n",
    "    - cache: (x, w, b)\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the affine forward pass. Store the result in out. You   #\n",
    "    # will need to reshape the input into rows.                               #\n",
    "    ###########################################################################\n",
    "    out = x @ w + b\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    cache = (x, w, b)\n",
    "    return out, cache\n",
    "\n",
    "def affine_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for an affine layer.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivative, of shape (N, M)\n",
    "    - cache: Tuple of:\n",
    "      - x: Input data, of shape (N, d_1, ... d_k)\n",
    "      - w: Weights, of shape (D, M)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\n",
    "    - dw: Gradient with respect to w, of shape (D, M)\n",
    "    - db: Gradient with respect to b, of shape (M,)\n",
    "    \"\"\"\n",
    "    x, w, b = cache\n",
    "    dx, dw, db = None, None, None\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the affine backward pass.                               #\n",
    "    ###########################################################################\n",
    "    dx = dout @ w.T\n",
    "    dw = x.T @ dout \n",
    "    db = dout.sum(axis=0)\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return dx, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.arange(10.0).reshape(2, 5)\n",
    "W = np.random.randn(5, 3)\n",
    "b = np.random.randn(3, )\n",
    "\n",
    "out, cache = affine_forward(X, W, b)\n",
    "dout = np.random.randn(2, 3)\n",
    "dx, dw, db = affine_backward(dout, cache)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_forward(X, W, b)[0], X, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda x: affine_forward(X, W, b)[0], W, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda x: affine_forward(X, W, b)[0], b, dout)\n",
    "\n",
    "rel_error(dx_num, dx) < 1e-5\n",
    "rel_error(dw_num, dw) < 1e-5\n",
    "rel_error(db_num, db) < 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_forward(x):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - x: Inputs, of any shape\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output, of the same shape as x\n",
    "    - cache: x\n",
    "    \"\"\"\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the ReLU forward pass.                                  #\n",
    "    ###########################################################################\n",
    "    out = x.copy()  # Must use copy in numpy to avoid pass by reference.\n",
    "    out = np.maximum(x, 0.0)\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    cache = x\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def relu_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - dout: Upstream derivatives, of any shape\n",
    "    - cache: Input x, of same shape as dout\n",
    "\n",
    "    Returns:\n",
    "    - dx: Gradient with respect to x\n",
    "    \"\"\"\n",
    "    dx, x = None, cache\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the ReLU backward pass.                                 #\n",
    "    ###########################################################################\n",
    "    #relu_mask = (x >= 0.0)\n",
    "    #dx = dout * relu_mask\n",
    "    dx = np.sign(np.maximum(x, 0)) * dout\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[-5.0, 1.0, 0.0, 13.0, 30.0], [6.0, -20, 0.0, 1.0, 0.0]]).reshape(2, 5)\n",
    "#dout = np.random.randn(2, 5)\n",
    "dout = np.array([[1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 10, 1.0, 1.0, 1.0]]).reshape(2, 5)\n",
    "out, cache = relu_forward(X)\n",
    "d_x = relu_backward(dout, cache)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(X)[0], X, dout)\n",
    "rel_error(dx_num, dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_loss(x, y):\n",
    "    \"\"\"\n",
    "    Computes the loss and gradient for softmax classification.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth\n",
    "      class for the ith input.\n",
    "    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
    "      0 <= y[i] < C\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss: Scalar giving the loss\n",
    "    - dx: Gradient of the loss with respect to x\n",
    "    \"\"\"\n",
    "    shifted_logits = x - np.max(x, axis=1, keepdims=True)\n",
    "    Z = np.sum(np.exp(shifted_logits), axis=1, keepdims=True)\n",
    "    log_probs = shifted_logits - np.log(Z)\n",
    "    probs = np.exp(log_probs)\n",
    "    N = x.shape[0]\n",
    "    loss = -np.sum(log_probs[np.arange(N), y]) / N\n",
    "    dx = probs.copy()\n",
    "    dx[np.arange(N), y] -= 1\n",
    "    dx /= N\n",
    "    return loss, dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "class MNIST:\n",
    "    def __init__(self, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        train, valid, test = self._load_data()\n",
    "        self.X_train, self.y_train = train[0], train[1]\n",
    "\n",
    "        # encoding y_train using one-hot encoding\n",
    "        #self.y_train_one_hot = np.zeros((self.y_train.shape[0], 10))\n",
    "        #self.y_train_one_hot[np.arange(self.y_train.shape[0]), self.y_train] = 1\n",
    "\n",
    "        self.X_valid, self.y_valid = valid[0], valid[1]\n",
    "        self.X_test, self.y_test = test[0], test[1]\n",
    "\n",
    "    def train_batch_generator(self):\n",
    "        while True:\n",
    "            rand_indices = np.random.choice(self.X_train.shape[0], self.batch_size, False)\n",
    "            yield self.X_train[rand_indices], self.y_train[rand_indices] #self.y_train_one_hot[rand_indices]\n",
    "\n",
    "    def validation(self):\n",
    "        return self.X_valid, self.y_valid\n",
    "\n",
    "    def testing(self):\n",
    "        return self.X_test, self.y_test\n",
    "\n",
    "    def num_features(self):\n",
    "        return self.X_train.shape[1]\n",
    "\n",
    "    def _load_data(self):\n",
    "        script_dir = os.path.dirname('.')\n",
    "        mnist_file = os.path.join(os.path.join(script_dir, 'data'), 'mnist.pkl.gz')\n",
    "\n",
    "        with gzip.open(mnist_file, 'rb') as mnist_file:\n",
    "            u = pickle._Unpickler(mnist_file)\n",
    "            u.encoding = 'latin1'\n",
    "            train, val, test = u.load()\n",
    "        return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = MNIST(batch_size=32)\n",
    "train_gen = mnist.train_batch_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, _ = next(train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, input_size=(1, 28, 28), num_filters=30, filter_size=5, output_size=10):\n",
    "        self.input_size = input_size\n",
    "        self.num_filters = num_filters \n",
    "        self.filter_size = filter_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        rand = np.random.RandomState(seed=1024)\n",
    "        self.W1 = rand.normal(scale=0.01, size=(num_filters, 1, filter_size, filter_size))\n",
    "        self.b1 = rand.normal(scale=0.01, size=(num_filters, 1))\n",
    "        \n",
    "        self.first_hid_h = int((28 - filter_size + 2*0) / 1) + 1\n",
    "        self.first_hid_w = int((28 - filter_size + 2*0) / 1) + 1\n",
    "        \n",
    "        \n",
    "        self.W2 = rand.normal(scale=0.01, size=(self.first_hid_h*self.first_hid_w*num_filters, output_size))\n",
    "        print(self.W2.shape)\n",
    "        self.b2 = rand.normal(scale=0.01, size=(output_size))\n",
    "        \n",
    "        self.mnist = MNIST(batch_size=64)        \n",
    "        \n",
    "    \n",
    "    def train(self, num_iter=250):\n",
    "        train_iter = self.mnist.train_batch_generator()\n",
    "        for i in range(num_iter):\n",
    "            X_train, y_train = next(train_gen)\n",
    "            X_train = X_train.reshape((-1, 1, 28, 28))\n",
    "            #print(X_train.shape)\n",
    "            #print(y_train.shape)\n",
    "            #print(self.W1.shape)\n",
    "            \n",
    "            # \n",
    "            out, conv_1_cache = conv_forward(X_train, self.W1, self.b1, padding=0)\n",
    "            out, relu_cache = relu_forward(out)    \n",
    "            #print(out.shape)\n",
    "            out = out.reshape(32, -1)\n",
    "            #print(out.shape)\n",
    "            out, affine_cache = affine_forward(out, self.W2, self.b2)\n",
    "            \n",
    "            loss, dout = softmax_loss(out, y_train)\n",
    "            if i % 250 == 0:\n",
    "                print(loss)\n",
    "            \n",
    "            dout, dW2, db2 = affine_backward(dout, affine_cache)\n",
    "            dout = dout.reshape(32, self.num_filters, self.first_hid_h, self.first_hid_w)\n",
    "            #print(dout.shape)\n",
    "            dout = relu_backward(dout, relu_cache)\n",
    "            dout, dW1, db1 = conv_backward(dout, conv_1_cache)\n",
    "            \n",
    "            \n",
    "            \n",
    "            self.W1 = self.W1 - 0.001*dW1\n",
    "            self.b1 = self.b1 - 0.001*db1\n",
    "            \n",
    "            self.W2 = self.W2 - 0.001*dW2\n",
    "            self.b2 = self.b2 - 0.001*db2\n",
    "        \n",
    "    \n",
    "    def test(self):\n",
    "        X_test, y_test = self.mnist.testing()\n",
    "        X_test = X_test.reshape((-1, 1, 28, 28))\n",
    "        print(X_test.shape)\n",
    "        out, conv_1_cache = conv_forward(X_test, self.W1, self.b1, padding=0)\n",
    "        out, relu_cache = relu_forward(out)    \n",
    "            #print(out.shape)\n",
    "        out = out.reshape(10000, -1)\n",
    "            #print(out.shape)\n",
    "        out, affine_cache = affine_forward(out, self.W2, self.b2)\n",
    "        \n",
    "        correct = np.sum(np.equal(y_test, np.argmax(out, axis=1)))\n",
    "        percentage = (correct / (y_test.shape[0])) * 100.00\n",
    "        print(percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17280, 10)\n",
      "2.3040088618\n",
      "2.28791685391\n",
      "2.24247573162\n",
      "2.03836845755\n",
      "1.5710882028\n",
      "1.12664798764\n",
      "0.712651984088\n",
      "0.620957181107\n",
      "0.505868698416\n",
      "0.515338445754\n",
      "0.628905424147\n",
      "0.294336287199\n",
      "0.329698561095\n",
      "0.444646784282\n",
      "0.271236667222\n",
      "0.429907827419\n",
      "0.369935553695\n",
      "0.945401472879\n",
      "0.321740119706\n",
      "0.362152374742\n",
      "(10000, 1, 28, 28)\n",
      "89.9\n"
     ]
    }
   ],
   "source": [
    "network = Network()\n",
    "network.train(num_iter=5000)\n",
    "network.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, input_size=28*28, output_size=10):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        rand = np.random.RandomState(seed=1024)\n",
    "        self.W1 = rand.normal(scale=0.01, size=(input_size, output_size))\n",
    "        self.b1 = rand.normal(scale=0.01, size=(output_size))\n",
    "              \n",
    "        self.mnist = MNIST(batch_size=64)        \n",
    "        \n",
    "    \n",
    "    def train(self, num_iter=250):\n",
    "        train_iter = self.mnist.train_batch_generator()\n",
    "        for i in range(num_iter):\n",
    "            X_train, y_train = next(train_gen) \n",
    "            out, affine_cache = affine_forward(X_train, self.W1, self.b1)\n",
    "            \n",
    "            loss, dout = softmax_loss(out, y_train)\n",
    "            if i % 250 == 0:\n",
    "                print(loss)\n",
    "            \n",
    "            dout, dW1, db1 = affine_backward(dout, affine_cache)            \n",
    "            \n",
    "            \n",
    "            self.W1 = self.W1 - 0.001*dW1\n",
    "            self.b1 = self.b1 - 0.001*db1\n",
    "            \n",
    "            #self.W2 = self.W2 - 0.001*dW2\n",
    "            #self.b2 = self.b2 - 0.001*db2\n",
    "        \n",
    "    \n",
    "    def test(self):\n",
    "        X_test, y_test = self.mnist.testing()\n",
    "        out, affine_cache = affine_forward(X_test, self.W1, self.b1)\n",
    "        \n",
    "        correct = np.sum(np.equal(y_test, np.argmax(out, axis=1)))\n",
    "        percentage = (correct / (y_test.shape[0])) * 100.00\n",
    "        print(percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.31531253936\n",
      "2.02714240171\n",
      "1.94361258532\n",
      "1.7470106729\n",
      "1.64754674547\n",
      "1.48137462908\n",
      "1.29335680744\n",
      "1.21296555738\n",
      "1.13913115393\n",
      "1.20271920262\n",
      "0.949475155889\n",
      "0.985219385182\n",
      "1.13710925994\n",
      "1.10417337316\n",
      "0.891957873001\n",
      "1.00166146398\n",
      "0.931458211235\n",
      "0.732332387641\n",
      "0.869039140127\n",
      "0.757504745317\n",
      "84.96\n"
     ]
    }
   ],
   "source": [
    "network = Network()\n",
    "network.train(num_iter=5000)\n",
    "network.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
